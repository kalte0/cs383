BMC CS 383: LLMs & Deep Learning
================================
Fine-Tuning GPT-2 for Simulating Student Responses on Standardized Tests
------------------------------------------------------------------------
This project explores Large Language Models (LLMs) and Deep Learning techniques, with a focus on fine-tuning a GPT-2 model on elementary-level standardized testing data. The goal is to simulate how a student might respond to various standardized test questions, evaluating the model's ability to generate human-like answers.

##### Methods & Approach
* Data Collection & Preprocessing

* Curating a dataset of standardized multiple-choice test questions and student-style responses

* Cleaning and formatting the data for optimal fine-tuning

##### Model Fine-Tuning

* Using Hugging Faceâ€™s Transformers library to fine-tune GPT-2

* Training with custom loss functions and optimized hyperparameters

##### Evaluation & Analysis

* Measuring accuracy (to grade-school level students) of generated responses

* Identifying potential biases and inconsistencies in the fine-tuned model

##### Technologies Used
* Python (for data preprocessing and training)

* PyTorch & TensorFlow (for deep learning)

* Hugging Face Transformers (for GPT-2 fine-tuning)

* Amazon Web Services (for experimentation and visualization)
