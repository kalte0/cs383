import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from torch.utils.data import Dataset, DataLoader

# Load NLTK stopwords
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

class SentimentDataset(Dataset):
    def __init__(self, texts, labels, max_length=512):
        self.texts = texts
        self.labels = labels
        self.max_length = max_length
        self.label_encoder = LabelEncoder()
        self.label_encoder.fit(self.labels)
        self.tokenizer = None  # Initialize tokenizer

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]

        # Preprocess text and tokenize
        tokens = self.preprocess_text(text)
        if self.tokenizer is None:
            self.tokenizer = self.build_tokenizer(tokens)

        # Convert tokens to indices using tokenizer
        token_ids = [self.tokenizer.get(token, self.tokenizer['<OOV>']) for token in tokens]

        # Pad or truncate token_ids to max_length
        if len(token_ids) < self.max_length:
            token_ids += [0] * (self.max_length - len(token_ids))
        else:
            token_ids = token_ids[:self.max_length]

        # Convert token_ids to tensor
        text_tensor = torch.tensor(token_ids, dtype=torch.long).to(device)  # Move tensor to device

        # Convert label to tensor
        label_tensor = torch.tensor(self.label_encoder.transform([label])[0], dtype=torch.long).to(device)  # Move tensor to device

        return text_tensor, label_tensor

    def preprocess_text(self, text):
        # Remove HTML tags
        text = re.sub(r'<[^>]+>', '', text)
        # Convert to lowercase
        text = text.lower()
        # Remove punctuation and numbers
        text = re.sub(r'[^a-zA-Z\s]', '', text)
        # Tokenize
        tokens = word_tokenize(text)
        # Remove stopwords and lemmatize
        stop_words = set(stopwords.words('english'))
        lemmatizer = WordNetLemmatizer()
        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
        return tokens

    def build_tokenizer(self, tokens):
        # Build tokenizer from tokens
        tokenizer = {token: idx + 1 for idx, token in enumerate(set(tokens))}
        # Add OOV token
        tokenizer['<OOV>'] = len(tokenizer) + 1
        return tokenizer

class SentimentClassifier(nn.Module):
    def __init__(self, embed_size, num_classes, max_pool=True):
        super(SentimentClassifier, self).__init__()
        # Transformer block (acting as the encoder)
        self.transformer = TransformerBlock(embed_size, forward_expansion=4)
        self.pool = max_pool
        # Output fully connected layer for classification
        self.fc = nn.Linear(embed_size, num_classes)

    def forward(self, x):
        # Ensure input tensor has correct shape and dtype
        if len(x.shape) == 2:  # If input tensor has shape (batch_size, sequence_length)
            x = x.unsqueeze(1)  # Add a singleton dimension for the sequence length
        x = x.float()  # Convert input tensor to float32 dtype
        # Pass input through the transformer block
        transformed = self.transformer(x.permute(0, 2, 1))  # Permute the input tensor

        # Apply global max pooling if specified
        if self.pool:
            pooled = torch.max(transformed, dim=1)[0]  # Max pooling across the sequence dimension
        else:
            # Keep only the output corresponding to the last token
            pooled = transformed[:, -1, :]

        # Pass through the output layer to get class logits
        out = self.fc(pooled)
        return out.to(device)  # Move tensor to device



class TransformerBlock(nn.Module):
    def __init__(self, embed_size, forward_expansion):
        super(TransformerBlock, self).__init__()
        # Self-attention layer
        self.attention = SelfAttention(embed_size)
        # Normalization layer 1
        self.norm1 = nn.LayerNorm(embed_size)
        # Feedforward layers
        self.feed_forward = nn.Sequential(
            nn.Linear(embed_size, forward_expansion * embed_size),
            nn.ReLU(),
            nn.Linear(forward_expansion * embed_size, embed_size),
        )
        # Normalization layer 2
        self.norm2 = nn.LayerNorm(embed_size)

    def forward(self, x):
        # Apply self-attention
        attention = self.attention(x)
        # Add & normalize (residual connection)
        x = self.norm1(attention + x)
        # Apply feedforward layers
        forward = self.feed_forward(x)
        # Add & normalize (residual connection)
        out = self.norm2(forward + x)
        return out

class SentimentClassifier(nn.Module):
    def __init__(self, embed_size, num_classes, max_pool=True):
        super(SentimentClassifier, self).__init__()
        # Transformer block (acting as the encoder)
        self.transformer = TransformerBlock(embed_size, forward_expansion=4)
        self.pool = max_pool
        # Output fully connected layer for classification
        self.fc = nn.Linear(embed_size, num_classes)

    def forward(self, x):
        # Ensure input tensor has correct shape and dtype
        x = x.float()  # Convert input tensor to float32 dtype
        # Reshape input tensor to have three dimensions
        x = x.unsqueeze(2)  # Add a singleton dimension for the embedding size
        # Pass input through the transformer block
        transformed = self.transformer(x.permute(0, 2, 1))  # Permute the input tensor

        # Apply global max pooling if specified
        if self.pool:
            pooled = torch.max(transformed, dim=1)[0]  # Max pooling across the sequence dimension
        else:
            # Keep only the output corresponding to the last token
            pooled = transformed[:, -1, :]

        # Pass through the output layer to get class logits
        out = self.fc(pooled)
        return out.to(device)  # Move tensor to device

# Load data
csv_file = "drive/MyDrive/IMDB Dataset.csv"
df = pd.read_csv(csv_file)

# Prepare dataset
texts = df['review'].tolist()
labels = df['sentiment'].tolist()

# Split dataset into training and validation sets
train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)

# Instantiate the dataset objects
train_dataset = SentimentDataset(train_texts, train_labels)
val_dataset = SentimentDataset(val_texts, val_labels)

# Define DataLoader for batch processing
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32)

# Instantiate the model
embed_size = 512  # Adjust the embed_size to match the output size of the TransformerBlock
num_classes = len(train_dataset.label_encoder.classes_)
model = SentimentClassifier(embed_size, num_classes)

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Training loop
num_epochs = 5
for epoch in range(num_epochs):
    model.train()
    for batch_inputs, batch_labels in train_loader:
        optimizer.zero_grad()

        # Move tensors to the appropriate device and convert to float
        batch_inputs = batch_inputs.to(device)
        batch_labels = batch_labels.to(device)

        # Pass the input to the model
        batch_outputs = model(batch_inputs)
        loss = criterion(batch_outputs, batch_labels)
        loss.backward()
        optimizer.step()

    # Evaluation
    model.eval()
    val_loss = 0.0
    correct = 0
    total = 0
    with torch.no_grad():
        for batch_inputs, batch_labels in val_loader:
            # Move tensors to the appropriate device and convert to float
            batch_inputs = batch_inputs.to(device)
            batch_labels = batch_labels.to(device)

            batch_outputs = model(batch_inputs)
            val_loss += criterion(batch_outputs, batch_labels).item()
            _, predicted = torch.max(batch_outputs, 1)
            total += batch_labels.size(0)
            correct += (predicted == batch_labels).sum().item()

    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {val_loss/len(val_loader)}, Accuracy: {(correct/total)*100}%")

Output (after running for 14 minutes): 
Epoch [1/5], Loss: 0.694786510528467, Accuracy: 50.839999999999996%
Epoch [2/5], Loss: 0.6943874755225623, Accuracy: 50.51%
Epoch [3/5], Loss: 0.6938558552211846, Accuracy: 51.2%
Epoch [4/5], Loss: 0.6936140304175429, Accuracy: 51.57000000000001%
Epoch [5/5], Loss: 0.6926364369285755, Accuracy: 51.78%
